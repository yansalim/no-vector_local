# No-Vector PDF Chat

A revolutionary PDF chatbot that uses **no vector embeddings** or traditional RAG. Instead, it leverages Large Language Models for intelligent document selection and page relevance detection, providing a completely stateless and privacy-first experience.

## ğŸš€ What Makes This "No-Vector"?

Traditional PDF chatbots convert documents into vector embeddings for semantic search. This approach:
- âŒ **Requires expensive vector databases**
- âŒ **Needs pre-processing and indexing**
- âŒ **Stores document data on servers**
- âŒ **Loses context and nuance in embeddings**

Our **No-Vector** approach:
- âœ… **Uses LLM reasoning instead of vectors**
- âœ… **Processes documents in real-time**
- âœ… **Completely stateless - no server storage**
- âœ… **Preserves full document context**
- âœ… **Privacy-first - documents stay in your browser**

## ğŸ§  How the No-Vector Process Works

### 3-Step Intelligent Document Analysis

```mermaid
graph TD
    A[ğŸ“„ User uploads PDFs] --> B[ğŸ§  LLM Document Selection]
    B --> C[ğŸ¯ LLM Page Relevance Detection]
    C --> D[ğŸ’¬ Contextual Answer Generation]
    
    B --> B1[Analyzes collection description<br/>+ document filenames<br/>+ user question]
    C --> C1[Examines actual page content<br/>from selected documents<br/>in parallel processing]
    D --> D1[Generates comprehensive answer<br/>with proper citations]
```

### Step 1: ğŸ§  **Smart Document Selection**
- LLM reads your collection description and document filenames
- Intelligently selects which documents are likely to contain relevant information
- No embeddings needed - uses reasoning and context understanding

### Step 2: ğŸ¯ **Page Relevance Detection**
- LLM examines actual page content from selected documents
- Processes multiple documents in parallel for speed
- Identifies the most relevant pages based on question context

### Step 3: ğŸ’¬ **Contextual Answer Generation**
- Uses only the relevant pages to generate accurate answers
- Maintains full document context and nuance
- Provides proper citations and references

## âœ¨ Key Features

### ğŸ”’ **Privacy-First & Stateless**
- **Zero Server Storage**: Documents processed and stored entirely in your browser
- **LocalStorage Persistence**: Your documents persist across browser sessions
- **No Data Leakage**: Document content never persists on servers


### ğŸ“ **Advanced File Handling**
- **Up to 100 PDF documents** per session
- **Chunked Upload System**: Automatically handles large file sets (>4.5MB)
- **4.5MB per file limit** - processes substantial documents
- **Real-time Processing**: No pre-indexing required

### ğŸ’¡ **Intelligent Processing**
- **Multi-Model Support**: GPT-5 GPT-5-mini, and more
- **Parallel Processing**: Multiple documents analyzed simultaneously
- **Context Preservation**: Full document context maintained throughout
- **Dynamic Descriptions**: Edit collection descriptions anytime

### ğŸ¨ **Modern Interface**
- **Responsive Design**: Works on desktop and mobile
- **Real-time Progress**: Visual feedback during uploads and processing
- **GitHub Integration**: Easy access to source code
- **Error Handling**: Comprehensive error messages and recovery

## ğŸ›  Technology Stack

### Frontend
- **React + Vite**: SPA build with lightning-fast dev server and static output
- **TypeScript**: Type safety and better DX
- **Tailwind CSS**: Utility-first styling (via PostCSS plugin)
- **Lucide React**: Clean, consistent icon set

### Backend (FastAPI)
- **FastAPI + Uvicorn**: High-performance Python API
- **PyPDF2**: Robust PDF text extraction
- **OpenAI**: LLM reasoning for doc/page selection and answers
- **Chunked Processing**: Efficient multi-file uploads

### Infrastructure
- **Docker Compose**: One command to run frontend (Nginx) and backend
- **No Databases**: Completely stateless architecture
- **Nginx**: Serves built React app and proxies API

## ğŸš€ Quick Start

### Prerequisites
- Node.js 18+ and npm
- Python 3.10+ (if you need to start the backend manually)
- Docker (optional, to run with containers)
- OpenAI API key (required for full functionality)

### 1. Clone and Install
```bash
git clone https://github.com/yansalim/no-vector_local.git
cd no-vector_local
npm install
```

### 2. Environment Setup
- Frontend (dev): uses `VITE_API_BASE_URL` to point to the backend
- Backend: uses `OPENAI_API_KEY` (required)

Create `.env` in the backend's root directory (or export the variable):
```bash
OPENAI_API_KEY=your_openai_api_key_here
```

### 3. Run Locally (with Node)
```bash
# starts frontend (Vite) + backend (Uvicorn) in parallel (Windows-friendly)
npm run dev

# frontend only (port 3000)
npm run web

# backend only (port 8000)
npm run backend:win
```
Visit http://localhost:3000 (or 3001 if 3000 is in use)

If the backend is at a different URL on your machine, export it before building the frontend:
```bash
set VITE_API_BASE_URL=http://localhost:8000 && npm run web
```

### 4. Running with Docker (recommended)
```bash
docker compose build --no-cache
docker compose up -d
```
Useful environment variables (via docker-compose.yml):
- `BACKEND_URL` (passed to the build as `VITE_API_BASE_URL` for the frontend)
- `OPENAI_API_KEY` (backend)

URLs:
- Frontend: http://localhost:3000
- Backend: http://localhost:8000/health

## ğŸ“– How to Use

### 1. **Upload Your Documents**
- Click "Add Your First Document" or "Add Files"
- Select up to 100 PDF files (4.5MB each max)
- Add a description of your document collection
- Large uploads are automatically chunked for reliability

### 2. **Start Chatting**
- Ask questions about your documents in natural language
- Watch the 3-step process: Document Selection â†’ Page Detection â†’ Answer Generation
- Get detailed answers with timing and cost breakdowns

### 3. **Manage Your Collection**
- Add more documents anytime
- Edit collection descriptions
- Start new sessions as needed
- All data stays in your browser

## ğŸ— Architecture Deep Dive

### Stateless Design
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Browser       â”‚    â”‚   FastAPI     â”‚    â”‚   OpenAI API    â”‚
â”‚                 â”‚    â”‚               â”‚    â”‚                 â”‚
â”‚ â€¢ LocalStorage  â”‚â—„â”€â”€â–ºâ”‚ â€¢ /upload     â”‚â—„â”€â”€â–ºâ”‚ â€¢ GPT Models    â”‚
â”‚ â€¢ Document Data â”‚    â”‚ â€¢ /chat/streamâ”‚    â”‚ â€¢ Real-time     â”‚
â”‚ â€¢ Chat History  â”‚    â”‚ â€¢ /health     â”‚    â”‚   Processing    â”‚
â”‚ â€¢ Session State â”‚    â”‚ â€¢ Stateless   â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Chunked Upload System
When uploading large document sets:
1. **Size Detection**: Frontend calculates total upload size
2. **Automatic Chunking**: Splits into 3.5MB chunks if needed
3. **Parallel Processing**: Each chunk processed independently
4. **Progressive Results**: Documents become available as chunks complete
5. **Error Recovery**: Failed chunks can be retried individually

## ğŸ”§ API Endpoints (FastAPI)

### `POST /upload`
Upload and process PDF documents
- **Input**: FormData with files and description
- **Output**: Processed documents with extracted text
- **Features**: Automatic chunking, progress tracking

### `POST /chat/stream`
Stream chat responses in real-time
- **Input**: Question, documents, chat history
- **Output**: Server-sent events with processing steps
- **Features**: Real-time progress, cost tracking, citations

### `GET /health`
Service health check
- **Output**: System status and mode information

## ğŸ¯ Advantages Over Traditional RAG

| Traditional RAG | No-Vector Approach |
|----------------|---------------------|
| ğŸ—„ï¸ Requires vector database | ğŸš« No database needed |
| ğŸ“Š Pre-processes to embeddings | ğŸ”„ Real-time processing |
| ğŸ’° Expensive infrastructure | ğŸ’¸ Serverless & cost-effective |
| ğŸ”’ Stores data on servers | ğŸ›¡ï¸ Browser-only storage |
| ğŸ“ Limited by embedding dimensions | ğŸ§  Full context understanding |
| âš¡ Fast retrieval, lossy context | ğŸ¯ Accurate reasoning, full context |

## ğŸŒŸ Example Workflow

1. **Upload**: Marketing team uploads 50 company PDFs
2. **Describe**: "Company policies, procedures, and guidelines"
3. **Ask**: "What is our remote work policy?"
4. **Process**:
   - ğŸ§  LLM selects "HR Handbook" and "Remote Work Guidelines"
   - ğŸ¯ Identifies relevant pages about remote work
   - ğŸ’¬ Generates comprehensive answer with citations
5. **Result**: Accurate answer in ~15 seconds with cost breakdown

## ğŸ”® Future Enhancements

- **Multi-format Support**: Word docs, PowerPoint, Excel
- **Advanced Citations**: Highlight exact text passages
- **Collaboration Features**: Share sessions with team members
- **Analytics Dashboard**: Usage patterns and insights
- **Custom Models**: Support for local and custom LLMs
- **Batch Operations**: Process multiple questions simultaneously

## ğŸ¤ Contributing

We welcome contributions! This project showcases how modern LLMs can replace traditional vector-based approaches while providing better accuracy and user experience.

## ğŸ“„ License

MIT License - see LICENSE file for details.

---

â­ **Star us on GitHub** if you find this no-vector approach interesting!

Built with â¤ï¸ by [ROE AI Inc.](https://github.com/yansalim/)